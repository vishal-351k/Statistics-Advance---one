{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Explain the properties of the F-distribution**\n",
        "\n",
        "The F-distribution is a crucial concept in statistics, particularly in the context of hypothesis testing and analysis of variance (ANOVA). Below are the key properties and characteristics of the F-distribution.\n",
        "\n",
        "## Definition and Characteristics\n",
        "- **Definition**: The F-distribution, also known as the Fisher-Snedecor distribution, is defined as the ratio of two independent chi-square random variables divided by their respective degrees of freedom. Specifically, if $ U_1 $ and $ U_2 $ are independent chi-square random variables with degrees of freedom $ d_1 $ and $ d_2$, then the random variable $ F $ defined as\n",
        "\n",
        "$$\n",
        "F = \\frac{U_1/d_1}{U_2/d_2}\n",
        "$$\n",
        "\n",
        "follows an F-distribution with parameters $ d_1 $ and $ d_2 $ .\n",
        "\n",
        "- **Range**: The values of the F-distribution are always **non-negative**, ranging from **0 to ∞**. This is due to the fact that variances (the components of the F-statistic) cannot be negative.\n",
        "\n",
        "## Properties\n",
        "- **Skewness**: The F-distribution is **positively skewed**, meaning it has a longer tail on the right side. As the degrees of freedom increase, this skewness decreases, making the distribution more symmetrical.\n",
        "\n",
        "- **Degrees of Freedom**: The shape of the F-distribution is influenced by its degrees of freedom:\n",
        "  - $ d_1 $: Degrees of freedom for the numerator.\n",
        "  - $ d_2 $: Degrees of freedom for the denominator.\n",
        "  The distribution becomes more symmetrical as both degrees of freedom increase.\n",
        "\n",
        "- **Reciprocal Property**: The F-distribution exhibits a reciprocal property where if $ F $ follows an F-distribution with degrees of freedom $ (d_1, d_2) $, then $ 1/F $ follows an F-distribution with degrees of freedom $ (d_2, d_1) $. This is particularly useful for determining critical values in statistical tests.\n",
        "\n",
        "- **Mean and Variance**:\n",
        "  - The mean of the F-distribution is given by:\n",
        "\n",
        "$$\n",
        "\\text{Mean} = \\frac{d_2}{d_2 - 2} \\quad (\\text{for } d_2 > 2)\n",
        "$$\n",
        "\n",
        "  - The variance is given by:\n",
        "\n",
        "$$\n",
        "\\text{Variance} = \\frac{2(d_2^2)(d_1 + d_2 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)} \\quad (\\text{for } d_2 > 4)\n",
        "$$\n",
        "These formulas highlight how the mean and variance depend on the degrees of freedom.\n",
        "\n",
        "## Applications\n",
        "The F-distribution is primarily used in:\n",
        "- **ANOVA**: To compare variances across multiple groups.\n",
        "- **F-tests**: To test hypotheses about variances or to compare two population variances.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euvFvPDUPbdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**\n",
        "\n",
        "\n",
        "\n",
        "The F-distribution is primarily used in several types of statistical tests, particularly those that involve comparing variances or assessing the significance of model parameters. Here are the main types of tests and analyses where the F-distribution is appropriate:\n",
        "\n",
        "a. **Analysis of Variance (ANOVA)**:\n",
        "\n",
        "* **One-Way ANOVA**: Used to test if there are significant differences between the means of three or more independent groups. The F-statistic is calculated as the ratio of the variance between the group means to the variance within the groups. If the null hypothesis (that all group means are equal) is true, the F-statistic follows an F-distribution.\n",
        "* **Two-Way ANOVA**: Extends one-way ANOVA to assess the impact of two independent categorical variables on a continuous outcome. It also uses F-tests to evaluate the significance of main effects and interactions.\n",
        "\n",
        "b. **Regression Analysis**:\n",
        "\n",
        "* In multiple regression, the F-test is used to determine whether the overall regression model is a good fit for the data. Specifically, it tests whether at least one of the predictor variables has a non-zero coefficient. The F-statistic is the ratio of the explained variance to the unexplained variance, and it follows an F-distribution under the null hypothesis that all regression coefficients are equal to zero.\n",
        "\n",
        "c. **Comparing Two Variances**:\n",
        "\n",
        "* The F-test can be used to compare the variances of two populations. This is often referred to as an F-test for equality of variances. The F-statistic is calculated as the ratio of the two sample variances. If the null hypothesis (that the variances are equal) is true, the statistic follows an F-distribution.\n",
        "\n",
        "d. **General Linear Models:**\n",
        "\n",
        "* The F-distribution is used in various extensions of linear models, including generalized linear models (GLMs) and mixed-effects models. In these contexts, F-tests can be used to evaluate the significance of model terms or to compare nested models.\n",
        "\n",
        "**Why the F-distribution is Appropriate**:\n",
        "\n",
        "a. **Ratio of Variances**: The F-distribution is specifically designed for situations where we are dealing with the ratio of two independent estimates of variance (e.g., between-group variance vs. within-group variance). This makes it particularly suitable for tests that compare variances or assess the significance of regression models.\n",
        "\n",
        "b. **Sampling Distribution**: Under the null hypothesis, the sampling distribution of the F-statistic can be shown to follow an F-distribution, which allows researchers to derive critical values and p-values for hypothesis testing.\n",
        "\n",
        "c. **Asymptotic Properties**: As sample sizes increase, the F-distribution approaches normality, making it robust for larger samples. This property is beneficial in practical applications where sample sizes can vary.\n",
        "\n",
        "d. **Flexibility with Degrees of Freedom**: The F-distribution is characterized by two degrees of freedom parameters, allowing it to model a wide range of scenarios involving different numbers of groups or predictors."
      ],
      "metadata": {
        "id": "wxp56YKBQkW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?**\n",
        "\n",
        "To conduct an F-test for comparing the variances of two populations, several key assumptions must be met to ensure the validity of the test results. These assumptions include:\n",
        "\n",
        "## Key Assumptions for the F-Test\n",
        "\n",
        "a. **Normality**:\n",
        "   - The populations from which the samples are drawn must be approximately normally distributed. This is crucial because the F-test is sensitive to deviations from normality; if the data significantly depart from a normal distribution, the results may be unreliable. Normality can be assessed using tests such as the Shapiro-Wilk test or visual methods like Q-Q plots.\n",
        "\n",
        "b. **Independence**:\n",
        "   - The samples must be independent of each other. This means that the selection of one sample should not influence the selection of the other. Independence is essential to ensure that the variances being compared are not affected by any underlying relationships between the samples.\n",
        "\n",
        "c. **Equality of Variances** (underlying null hypothesis):\n",
        "   - The null hypothesis for the F-test states that the population variances are equal (i.e., $H_0: \\sigma_1^2 = \\sigma_2^2$). While this is what we are testing, it is assumed that under the null hypothesis, this condition holds true for proper interpretation of results.\n",
        "\n",
        "d. **Random Sampling**:\n",
        "   - The samples should be randomly selected from their respective populations. This helps ensure that the samples are representative of their populations and reduces bias in the results.\n",
        "\n",
        "e. **Sample Size**:\n",
        "   - While not a strict assumption, having sufficiently large sample sizes can help meet the normality assumption due to the Central Limit Theorem, which states that sample means will tend to be normally distributed as sample size increases.\n",
        "\n"
      ],
      "metadata": {
        "id": "FVGr31d9SWG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. What is the purpose of ANOVA, and how does it differ from a t-test?**\n",
        "\n",
        "**Purpose of ANOVA (Analysis of Variance):**\n",
        "\n",
        "ANOVA (Analysis of Variance) is a statistical method used to determine whether there are statistically significant differences between the means of three or more independent groups. The primary purposes of ANOVA include:\n",
        "\n",
        "a. **Comparison of Means**: ANOVA assesses whether at least one group mean is significantly different from others among multiple groups, helping to identify any potential effects of one or more independent variables.\n",
        "\n",
        "b. **Control of Type I Error**: Unlike multiple t-tests, which increase the risk of Type I error (false positives), ANOVA allows for simultaneous comparison of multiple groups while maintaining the overall alpha level.\n",
        "\n",
        "c. **Evaluation of Variability**: ANOVA evaluates the variance within groups and the variance between groups. A significant F-statistic in ANOVA indicates that the variation between the group means is greater than what can be attributed to random chance, implying that at least one group is different.\n",
        "\n",
        "### How ANOVA Differs from a t-test:\n",
        "\n",
        "**a. Number of Groups:**\n",
        "   - **ANOVA**: Used for comparing means across three or more groups.\n",
        "   - **t-test**: Used for comparing means between two groups.\n",
        "\n",
        "**b. Hypothesis Testing Framework:**\n",
        "   - **ANOVA**: Tests the null hypothesis that all group means are equal($H0: μ1 = μ2 = μ3 = ... = μk$) against the alternative hypothesis that at least one mean is different.\n",
        "   - **t-test**: Tests the null hypothesis that the means of two groups are equal($H0: μ1 = μ2$) against the alternative that they are not equal.\n",
        "\n",
        "**c. Risk of Type I Error:**\n",
        "   - **ANOVA**: Controls the Type I error rate more effectively when comparing multiple groups because it evaluates all groups in a single analysis.\n",
        "   - **t-test**: Conducting multiple t-tests for more than two groups can lead to an inflated Type I error rate, increasing the likelihood of finding false positives.\n",
        "\n",
        "**d. F-statistic vs. t-statistic:**\n",
        "   - **ANOVA**: Uses the F-statistic, which represents the ratio of variance between groups to the variance within groups.\n",
        "   - **t-test**: Uses the t-statistic, which compares the difference in sample means relative to the variability among the samples.\n",
        "\n",
        "**e. Post-Hoc Testing:**\n",
        "   - **ANOVA**: If ANOVA reveals significant differences, post-hoc tests (e.g., Tukey, Bonferroni) are usually performed to identify which specific groups differ from each other.\n",
        "   - **t-test**: If the t-test shows a significant difference, analysis typically ends there unless additional comparisons are warranted.\n",
        "\n"
      ],
      "metadata": {
        "id": "AdcI8U_6TL-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.**\n",
        "\n",
        "When comparing the means of more than two groups, a one-way ANOVA is often preferred over multiple t-tests for several reasons, primarily related to statistical validity and efficiency.\n",
        "\n",
        "## When to Use One-Way ANOVA\n",
        "\n",
        "One-way ANOVA is appropriate when:\n",
        "- we have **three or more independent groups** that we want to compare.\n",
        "- we are interested in assessing the effect of a single independent variable (factor) on a continuous dependent variable.\n",
        "- The data meets the assumptions of normality and homogeneity of variances across groups.\n",
        "\n",
        "## Why Use One-Way ANOVA Instead of Multiple t-Tests\n",
        "\n",
        "a. **Control of Type I Error Rate**:\n",
        "   - Conducting multiple t-tests increases the risk of committing a Type I error (incorrectly rejecting the null hypothesis). For example, if we perform five t-tests at a significance level of 0.05, the overall probability of making at least one Type I error increases significantly. ANOVA addresses this by providing a single test that evaluates all group means simultaneously, maintaining a controlled Type I error rate across the entire analysis.\n",
        "\n",
        "b. **Efficiency**:\n",
        "   - One-way ANOVA is more efficient than performing multiple t-tests because it combines comparisons into one analysis. This reduces the number of tests performed and simplifies interpretation. Instead of running separate tests for each pair of groups, ANOVA evaluates all groups at once, which is particularly beneficial when dealing with many groups.\n",
        "\n",
        "c. **Statistical Power**:\n",
        "   - ANOVA can provide more statistical power than multiple t-tests because it uses information from all groups to estimate variance. This results in a more accurate assessment of differences among means by pooling variances across groups, leading to better estimates of standard errors and potentially smaller p-values when true differences exist.\n",
        "\n",
        "d. **Post-Hoc Analysis**:\n",
        "   - If ANOVA indicates significant differences among group means, post-hoc tests (e.g., Tukey's HSD) can be conducted to identify which specific groups differ from each other without inflating the Type I error rate. This structured approach allows for comprehensive analysis following an initial test.\n",
        "\n",
        "e. **Assumption Testing**:\n",
        "   - While both ANOVA and t-tests assume normality and homogeneity of variances, ANOVA's design allows for better handling of these assumptions through its overall framework. If assumptions are violated, adjustments or alternative methods can be applied more systematically.\n",
        "\n"
      ],
      "metadata": {
        "id": "n59Zpk93UR_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?**\n",
        "\n",
        "In ANOVA (Analysis of Variance), variance is partitioned into two main components: **between-group variance** and **within-group variance**. This partitioning is fundamental to the calculation of the F-statistic, which is used to determine whether there are significant differences among group means.\n",
        "\n",
        "## Partitioning of Variance in ANOVA\n",
        "\n",
        "a. **Total Variance**:\n",
        "   - The total variance in the data is calculated as the sum of the squared differences between each observation and the overall mean. This total variance can be expressed mathematically as:\n",
        "\n",
        "     $$\n",
        "     \\text {Total Sum of Squares (SST)} = \\sum_{i=1}^{N} (X_i - \\bar{X})^2\n",
        "     $$\n",
        "\n",
        "   where $X_i$ represents each observation, $\\bar{X}$ is the overall mean, and $N$ is the total number of observations.\n",
        "\n",
        "b. **Between-Group Variance**:\n",
        "   - This component measures how much the group means deviate from the overall mean. It reflects the variability due to the interaction between different groups. The formula for calculating between-group variance (also known as the Sum of Squares Between or SSB) is:\n",
        "\n",
        "     $$\n",
        "     \\text{SSB} = \\sum_{j=1}^{k} n_j (\\bar{X}_j - \\bar{X})^2\n",
        "     $$\n",
        "\n",
        "   where $k$ is the number of groups, $n_j$ is the sample size for group $j$, $\\bar{X}_j$ is the mean of group $j$, and $\\bar{X}$ is the overall mean.\n",
        "\n",
        "c. **Within-Group Variance**:\n",
        "   - This component measures variability within each group and reflects how much individual observations within each group deviate from their respective group mean. It is calculated as:\n",
        "\n",
        "     $$\n",
        "     \\text{SSW} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\bar{X}_j)^2\n",
        "     $$\n",
        "\n",
        "   where $X_{ij}$ represents each observation in group $j$, and $\\bar{X}_j$ is the mean of that group.\n",
        "\n",
        "d. **Relationship Between Variances**:\n",
        "   - The total variance can be expressed as:\n",
        "     $$\n",
        "     SST = SSB + SSW\n",
        "     $$\n",
        "   This equation shows that the total variability in the data can be decomposed into variability explained by differences between groups and variability within groups.\n",
        "\n",
        "## Contribution to F-Statistic Calculation\n",
        "\n",
        "The F-statistic in ANOVA is calculated as a ratio of these two variances:\n",
        "\n",
        "$$\n",
        "F = \\frac{\\text{Mean Square Between (MSB)}}{\\text{Mean Square Within (MSW)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- **Mean Square Between (MSB)** is calculated as:\n",
        "  $$\n",
        "  MSB = \\frac{\\text{SSB}}{df_B}\n",
        "  $$\n",
        "  with $df_B = k - 1$(degrees of freedom for between groups).\n",
        "\n",
        "- **Mean Square Within (MSW)** is calculated as:\n",
        "  $$\n",
        "  MSW = \\frac{\\text{SSW}}{df_W}\n",
        "  $$\n",
        "  with $df_W = N - k$ (degrees of freedom for within groups).\n",
        "\n",
        "The F-statistic thus represents how much greater the variability between group means is compared to the variability within groups. A larger F-value indicates that a significant portion of the total variance can be attributed to differences between group means rather than random variation within groups.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y6mL5gAYVGOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**\n",
        "\n",
        "The comparison between the classical (frequentist) approach to ANOVA and the Bayesian approach reveals significant differences in how each framework handles uncertainty, parameter estimation, and hypothesis testing. Here’s a detailed overview of these key differences:\n",
        "\n",
        "## a. Handling Uncertainty\n",
        "\n",
        "### Frequentist Approach\n",
        "- **Confidence Intervals**: Frequentist methods quantify uncertainty through confidence intervals. These intervals provide a range of values within which the true parameter (e.g., mean) is expected to lie with a certain level of confidence (e.g., 95%). However, this interpretation can be misleading; it does not provide the probability that the parameter lies within that interval after observing the data.\n",
        "- **P-Values**: The frequentist approach uses p-values to assess the strength of evidence against the null hypothesis. A low p-value indicates that observing the data under the null hypothesis is unlikely, but it does not directly quantify uncertainty about the hypothesis itself.\n",
        "\n",
        "### Bayesian Approach\n",
        "- **Posterior Distributions**: Bayesian methods handle uncertainty by calculating posterior distributions, which represent updated beliefs about parameters after observing data. This allows for direct probability statements about parameters (e.g., \"There is a 95% probability that the true mean is greater than a specific value\").\n",
        "- **Credible Intervals**: Instead of confidence intervals, Bayesian analysis uses credible intervals, which provide a range of values that contain the parameter with a specified probability, offering a more intuitive interpretation of uncertainty.\n",
        "\n",
        "## b. Parameter Estimation\n",
        "\n",
        "### Frequentist Approach\n",
        "- **Point Estimates**: Frequentist methods typically focus on point estimates (e.g., sample means) and rely on sampling distributions to make inferences about population parameters. The estimates are considered fixed and do not incorporate prior beliefs or information.\n",
        "- **Assumption of Fixed Parameters**: In frequentist statistics, parameters are treated as fixed but unknown quantities. The analysis focuses on estimating these parameters based solely on the observed data.\n",
        "\n",
        "### Bayesian Approach\n",
        "- **Prior Distributions**: Bayesian inference combines prior beliefs about parameters with observed data to produce posterior distributions. This allows for incorporating existing knowledge or expert opinion into the analysis.\n",
        "- **Dynamic Parameter Estimation**: Parameters are treated as random variables with associated probability distributions. This approach enables updating beliefs as new data becomes available.\n",
        "\n",
        "## c. Hypothesis Testing\n",
        "\n",
        "### Frequentist Approach\n",
        "- **Null Hypothesis Testing**: Frequentist ANOVA involves formulating a null hypothesis (e.g., all group means are equal) and an alternative hypothesis. The test assesses whether there is enough evidence to reject the null hypothesis based on p-values derived from F-statistics.\n",
        "- **Single Hypothesis Focus**: Frequentist methods typically test one hypothesis at a time, leading to concerns about Type I error rates when multiple comparisons are made.\n",
        "\n",
        "### Bayesian Approach\n",
        "- **Bayesian Hypothesis Testing**: Bayesian methods do not rely on fixed null and alternative hypotheses. Instead, they evaluate the probabilities of different hypotheses given the observed data using Bayes' theorem.\n",
        "- **Bayes Factor**: The Bayesian approach often employs Bayes factors to compare hypotheses, quantifying how much more likely one hypothesis is compared to another based on the data. This allows for a more flexible assessment of multiple hypotheses simultaneously.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tEneDUM0ZCPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Question: You have two sets of data representing the incomes of two different professions:**\n",
        "* **Profession A:** [48, 52, 55, 60, 62]\n",
        "* **Profession B:** [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are the conclusions based on the F-test?\n",
        "\n",
        "**Task:** Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "**Objective:** Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "BBXMdUNdaJa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for two professions\n",
        "professionA = np.array([48, 52, 55, 60, 62])\n",
        "professionB = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate variances\n",
        "varA = np.var(professionA, ddof=1)\n",
        "varB = np.var(professionB, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "fStatistic = varA / varB\n",
        "\n",
        "# Degrees of freedom\n",
        "dfA = len(professionA) - 1\n",
        "dfB = len(professionB) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "pValue = stats.f.sf(fStatistic, dfA, dfB)\n",
        "\n",
        "# Display results\n",
        "print(f\"Variance of Profession A: {varA:.2f}\")\n",
        "print(f\"Variance of Profession B: {varB:.2f}\")\n",
        "print(f\"F-statistic: {fStatistic:.2f}\")\n",
        "print(f\"P-value: {pValue:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ehM2pxbplL",
        "outputId": "0acb145a-695c-43a4-ed3d-929d2bd30cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of Profession A: 32.80\n",
            "Variance of Profession B: 15.70\n",
            "F-statistic: 2.09\n",
            "P-value: 0.2465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion**\n",
        "To interpret the results:\n",
        "* **F-statistic**: This value indicates how much larger the variance is in one profession compared to another.\n",
        "\n",
        "* **P-value**: If this value is less than the significance level (commonly set at 0.05), we would reject the null hypothesis that the variances are equal.\n",
        "\n",
        "If we find:\n",
        "* The p-value is greater than 0.05, we would conclude that there is not enough evidence to suggest that the variances are significantly different.\n",
        "\n",
        "* The p-value is less than or equal to 0.05, we would conclude that there is significant evidence to suggest that the variances are different."
      ],
      "metadata": {
        "id": "k3GHLBKndPQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:**\n",
        "* **Region A**: [160, 162, 165, 158, 164]\n",
        "* **Region B**: [172, 175, 170, 168, 174]\n",
        "* **Region C**: [180, 182, 179, 185, 183]\n",
        "* **Task**: Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "* **Objective**: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "imXLMcxWeSyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for the three regions\n",
        "regionA = np.array([160, 162, 165, 158, 164])\n",
        "regionB = np.array([172, 175, 170, 168, 174])\n",
        "regionC = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "fStatistic, pValue = stats.f_oneway(regionA, regionB, regionC)\n",
        "\n",
        "# Display results\n",
        "print(f\"F-statistic: {fStatistic:.4f}\")\n",
        "print(f\"P-value: {pValue:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if pValue < alpha:\n",
        "    print(\"Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There are no statistically significant differences in average heights between the regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94apKqMNgh3g",
        "outputId": "fe2bf2e5-b260-445b-c9fb-74538eec8428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.8733\n",
            "P-value: 0.0000\n",
            "Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Results**\n",
        "\n",
        "* **F-statistic**: This value indicates how much variance exists between group means compared to within-group variance.\n",
        "\n",
        "* **P-value**: If this value is less than the significance level (commonly set at 0.05), we would reject the null hypothesis that there are no differences in average heights among the regions.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "* If we find that p-value < 0.05, we conclude that there are statistically significant differences in average heights among at least one pair of regions.\n",
        "\n",
        "* If p-value ≥ 0.05, we conclude that there are no statistically significant differences in average heights among the regions."
      ],
      "metadata": {
        "id": "bjKmd27ohFOi"
      }
    }
  ]
}